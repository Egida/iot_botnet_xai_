{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "import pandas  as pd\n",
    "from pathlib import  Path\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "#df = pd.read_csv(Path().joinpath('data','nboit.csv'))\n",
    "#%%\n",
    "#1. data reading 2. label encoding 3.  smote data balancing.\n",
    "def datareading(folder_name,file_name,class_name,sample_size, x_numbers):\n",
    "    '''\n",
    "    Folder name:It reads the folder name\n",
    "    File Name  : Data File name, it is csv file. \n",
    "    class_name : class_name, it should class-1, class-2, class-3, class-4\n",
    "    sample_size : sample size must be label equally distribution\n",
    "    x_numbers : number of features, for Medbiot=100, N-BaIoT 115\n",
    "    '''\n",
    "    print(\"==\"*40)\n",
    "    data = pd.read_csv(Path().joinpath(folder_name, file_name))\n",
    "    class_name = data[class_name].name\n",
    "    if len(data[class_name].unique()) ==2:\n",
    "        print(\"Binary Classification\")\n",
    "    else:\n",
    "        print(\"Multi Class classification\")\n",
    "    print(f\"class_name:{class_name}\")\n",
    "    print(f\"class labels:{data[class_name].unique()}\")\n",
    "    df = data.groupby(class_name).apply(lambda x: x.sample(n=sample_size)).reset_index(drop=True)\n",
    "    le = LabelEncoder()\n",
    "    cols = df.columns.to_list()\n",
    "    for column in cols:\n",
    "        if df[column].name==class_name:\n",
    "            df[column] = le.fit_transform(df[column])\n",
    "    # data samples\n",
    "    X = df.iloc[:, 0:x_numbers]\n",
    "    y = df[class_name]\n",
    "    #X_sample, y_sample = Classbalancing(X, y).smote_balancing()\n",
    "    print(\"==\"*40)\n",
    "    return X, y\n",
    "#%%\n",
    "# X, y = datareading('data','nboit.csv','class-3')\n",
    "# print(y.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Multi Class classification\n",
      "class_name:class-2\n",
      "class labels:['mirai_attacks' 'benign_traffic' 'gafgyt_attacks']\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "folder_name = '/gpfs/mariana/home/rkalak/dataset/N-Balot/'\n",
    "file_name = 'nboit_sample.csv'\n",
    "class_name = 'class-2'\n",
    "sample_size = 4000\n",
    "x_numbers  = 115\n",
    "X,y  = datareading(folder_name,file_name,class_name,sample_size,x_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (6400, 115)\n",
      "y_train:(6400,)\n",
      "X_test: (1600, 115)\n",
      "y_test:(1600,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,stratify=y,random_state=42,test_size=0.2)\n",
    "print(\"X_train: {0}\\ny_train:{1}\\nX_test: {2}\\ny_test:{3}\".format(X_train.shape,\n",
    "                                                                  y_train.shape,\n",
    "                                                                  X_test.shape,\n",
    "                                                                  y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# class testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 0.1.36ubuntu1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 0.23ubuntu1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import mean\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "from sklearn.ensemble import ExtraTreesClassifier, GradientBoostingClassifier\n",
    "\n",
    "\n",
    "class parameter_tuning:\n",
    "    \"\"\"\n",
    "    Tuning Algorithms\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, y, metric_type, file_location, search_type='grid_search'):\n",
    "        \"\"\"\n",
    "        Tuning the algorithms\n",
    "\n",
    "        :param file_location: resultant file location.\n",
    "        :param metric_type: 1. accuracy, f1-score, recall, precision\n",
    "        :param cv: Cross validation score\n",
    "        :param search_type: Hyper Parameter Tuning type 1. Grid Search 2. Random Search type\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.X = X,\n",
    "        self.y = y,\n",
    "        self.metric_type = metric_type,\n",
    "        self.search_type = search_type,\n",
    "        self.file_location = file_location\n",
    "\n",
    "    def _fit_grid_random_search(self, ml_classifier, parameters):\n",
    "        \"\"\" Training the model using Grid search or Random search hyperparameter tuning methods.\n",
    "        :param X: Independent Variable\n",
    "        :param y: Dependent Variable\n",
    "        :param ml_classifier: Scikit learn classifier\n",
    "        :param parameters: various combinations for parameters for classifier.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        metric_type = self.metric_type[0]\n",
    "        print(\"metric:{0}\\n{1}\".format(metric_type, type(metric_type)))\n",
    "        cv_results_df = pd.DataFrame()\n",
    "        print(\"Tuning Type:{0}\\n\".format(self.search_type))\n",
    "        # Classifier name\n",
    "        mlclassifier_name = str(type(ml_classifier)).split(\".\")[-1][:-2]\n",
    "        print(\"Classifier is {0}\".format(mlclassifier_name))\n",
    "        X = self.X[0]\n",
    "        y = self.y[0]\n",
    "        # data shape\n",
    "        print(\"X variable: {0}\\ny Variable:{1}\".format(X.shape, y.shape))\n",
    "        # check the Parameter type,\n",
    "        cv = KFold(n_splits=5, random_state=100, shuffle=True)\n",
    "        search_type = self.search_type[0]\n",
    "        print(\"Search Type:{0}\\n\".format(search_type))\n",
    "        # Grid search tuning.\n",
    "        if search_type == 'grid_search':\n",
    "            # Grid Search parameter type\n",
    "            tuned_model = GridSearchCV(ml_classifier,\n",
    "                                       param_grid=parameters,\n",
    "                                       scoring=metric_type,\n",
    "                                       verbose=10,\n",
    "                                       refit=False)\n",
    "            start_time = self.timer(0)\n",
    "            tuned_model.fit(X, y)\n",
    "            finishing_time = self.timer(start_time)\n",
    "            print(\"Best parameters:{0}\".format(tuned_model.best_params_))\n",
    "            file_name = f'{self.file_location}/{mlclassifier_name}.pkl'\n",
    "            joblib.dump(tuned_model, file_name)\n",
    "            # saving the logs of model into a text file\n",
    "            df = self.res_logs_text_file(mlclassifier_name,\n",
    "                                         tuned_model,\n",
    "                                         finishing_time,\n",
    "                                         self.file_location)\n",
    "\n",
    "            return cv_results_df.append(df)\n",
    "        # random search\n",
    "        elif search_type == 'random_search':\n",
    "            # Random Search Parameter Tuning\n",
    "            tuned_model = RandomizedSearchCV(estimator=ml_classifier,\n",
    "                                             param_distributions=parameters,\n",
    "                                             scoring=metric_type,\n",
    "                                             return_train_score=True, \n",
    "                                             cv=cv,\n",
    "                                             verbose=10, refit='AUC')\n",
    "            # Tuning the model\n",
    "            start_time = self.timer(0)\n",
    "            model = tuned_model.fit(X, y)\n",
    "            finishing_time = self.timer(start_time)\n",
    "\n",
    "            file_name = f'{self.file_location}/{mlclassifier_name}.pkl'\n",
    "            \n",
    "            joblib.dump(model.best_estimator_, file_name)\n",
    "\n",
    "            print(\"==\"*40)\n",
    "            print(\"Best parameters:{0}\".format(model.best_params_))\n",
    "            print(\"Best Estimator:{0}\".format(model.best_estimator_))\n",
    "            print(\"Best score:{0}\".format(model.best_score_))\n",
    "            print(\"==\"*40)\n",
    "            # saving the logs of model into a text file\n",
    "            df = self.res_logs_text_file(mlclassifier_name,\n",
    "                                         model,\n",
    "                                         finishing_time,\n",
    "                                         self.file_location)\n",
    "            return cv_results_df.append(df)\n",
    "        else:\n",
    "            print(\"===========================================\")\n",
    "            print(f'{search_type} is wrong key word.'\n",
    "                  f'Key word should be either 1.grid_search or 2.random_search')\n",
    "\n",
    "        # save the model\n",
    "        return cv_results_df\n",
    "\n",
    "    def res_logs_text_file(self, mlclassifier_name, tuned_model, finish_time, file_location):\n",
    "        \"\"\"\n",
    "        saving the result into a text files\n",
    "        :param file_location: save resultant file location name. it must be with dataset name\n",
    "        :param mlclassifier_name: classifier name\n",
    "        :param tuned_model:  trained model\n",
    "        :param finish_time: model finishing time\n",
    "        :return: dataframe\n",
    "        \"\"\"\n",
    "        with open(f'{file_location}/parameter_tuning.txt', 'a') as res_logs:\n",
    "            res_logs.write('==' * 40)\n",
    "            res_logs.write(\"\\n\")\n",
    "            res_logs.write(\"1.Classifier:{0}\\n\".format(mlclassifier_name))\n",
    "            res_logs.write(\"2.Best Parameters:{0}\\n\".format(str(tuned_model.best_params_)))\n",
    "            res_logs.write(\"3.Duration:{0}\\n\".format(str(finish_time)))\n",
    "            res_logs.write('4.Accuracy: %.5f ' % (tuned_model.best_score_ * 100 ))\n",
    "            res_logs.write(\"\\n5.Best Estimator{0}\\n\".format(str(tuned_model.best_estimator_)))\n",
    "            res_logs.write('\\n')\n",
    "            res_logs.write('==' * 40)\n",
    "            res_logs.write('\\n')\n",
    "\n",
    "        # cv results\n",
    "        cv_results_df = pd.DataFrame(tuned_model.cv_results_)\n",
    "\n",
    "        # save the model\n",
    "        file_name = f'{self.file_location}/{mlclassifier_name}.pkl'\n",
    "        joblib.dump(tuned_model, file_name)\n",
    "\n",
    "        return cv_results_df\n",
    "\n",
    "    # Time to  count the model for training.\n",
    "    @staticmethod\n",
    "    def timer(start_time=None):\n",
    "        \"\"\"\n",
    "\n",
    "        :param start_time: 0\n",
    "        :return: Completion time\n",
    "        \"\"\"\n",
    "        time_list = []\n",
    "        if not start_time:\n",
    "            start_time = datetime.now()\n",
    "            return start_time\n",
    "        elif start_time:\n",
    "            thour, temp_sec = divmod(\n",
    "                (datetime.now() - start_time).total_seconds(), 3600)\n",
    "            tmin, tsec = divmod(temp_sec, 60)\n",
    "            # time_list.append(thour)\n",
    "            # print(\"\\n Time taken: %i hours %i minutes and %s seconds\" % (thour, tmin, round(tsec,2)))\n",
    "        return str(\"Time consumption: %i hours %i minutes and %s seconds\" % (thour, tmin, round(tsec, 2)))\n",
    "\n",
    "    def rf_classification(self):\n",
    "        \"\"\"\n",
    "        Random Forest Classifier\n",
    "        \"\"\"\n",
    "        # Initiate the classifier\n",
    "        classifier = RandomForestClassifier(n_jobs=-1)\n",
    "        # parameters\n",
    "        rf_params = {\n",
    "            'max_features': ['sqrt', 'auto', 'log2', None],\n",
    "            'max_depth': list(range(5, 51)),\n",
    "            'min_samples_leaf': list(range(1, 16)),\n",
    "            'min_samples_split': list(range(2, 31)),\n",
    "            'criterion': ['gini', 'entropy'],\n",
    "            'random_state': [100]\n",
    "        }\n",
    "\n",
    "        print(\"Tuning Type:{0}\\n\".format(self.search_type))\n",
    "        print(\"Classifier name:{0}\\n\".format(classifier.__class__.__name__))\n",
    "        for key, value in rf_params.items():\n",
    "            print(\"{0}:{1}\".format(key, value))\n",
    "        # parameters for grid search\n",
    "        # fitting the grid search or random search\n",
    "        cv_results = self._fit_grid_random_search(classifier, rf_params)\n",
    "        return cv_results\n",
    "\n",
    "    def dt_classification(self):\n",
    "        \"\"\"\n",
    "        Decision Tree Classifier\n",
    "        \"\"\"\n",
    "        # Initiate the classifier\n",
    "        classifier = DecisionTreeClassifier()\n",
    "        # parameters\n",
    "        dt_params = {\n",
    "            'max_features': ['sqrt', 'auto', 'log2', None],\n",
    "            'max_depth': list(range(5, 51)),\n",
    "            'min_samples_leaf': list(range(1, 16)),\n",
    "            'min_samples_split': list(range(2, 31)),\n",
    "            'criterion': ['gini', 'entropy'],\n",
    "            'random_state': [100]\n",
    "        }\n",
    "\n",
    "        # print(\"Tuning Type:{0}\\n\".format(self.search_type))\n",
    "        # print(\"Classifier name:{0}\\n\".format(classifier.__class__.__name__))\n",
    "        # for key, value in dt_params.items():\n",
    "        #     print(\"{0}:{1}\".format(key, value))\n",
    "        # parameters for grid search\n",
    "        # fitting the grid search or random search\n",
    "        cv_results = self._fit_grid_random_search(classifier, dt_params)\n",
    "        return cv_results\n",
    "\n",
    "    def knn_classification(self):\n",
    "        \"\"\"\n",
    "K-nearest neighbor classification\n",
    "        \"\"\"\n",
    "        # Initiate the classifier\n",
    "        classifier = KNeighborsClassifier(n_jobs=-1)\n",
    "        # parameters\n",
    "        k_range = list(range(1, 31))\n",
    "        knn_params = {\n",
    "            'n_neighbors': list(range(1, 21, 1)),\n",
    "            'weights': ['uniform', 'distance'],\n",
    "            'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "        }\n",
    "        # print(\"Tuning Type:{0}\\n\".format(self.search_type))\n",
    "        # print(\"Classifier name:{0}\\n\".format(classifier.__class__.__name__))\n",
    "        for key, value in knn_params.items():\n",
    "            print(\"{0}:{1}\".format(key, value))\n",
    "        # parameters for grid search\n",
    "        # fitting the grid search or random search\n",
    "        cv_results = self._fit_grid_random_search(classifier, knn_params)\n",
    "        return cv_results\n",
    "\n",
    "    def xgboost_classification(self):\n",
    "        \"\"\"\n",
    "xgboost\n",
    "        \"\"\"\n",
    "        xgb_params = rf_params = {\n",
    "            'num_leaves': sp_randint(6, 50),\n",
    "            'min_child_samples': sp_randint(100, 500),\n",
    "            'learning_rate': list(np.arange(0, 1.1, 0.4)),\n",
    "            'max_depth': list(range(5, 51, 5)),\n",
    "            'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n",
    "            'subsample': sp_uniform(loc=0.2, scale=0.8),\n",
    "            'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n",
    "            'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n",
    "            'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]\n",
    "        }\n",
    "        xgb_classifier = xgb.XGBClassifier(objective='binary:logistic',\n",
    "                                           use_label_encoder=False,\n",
    "                                           random_state=100)\n",
    "\n",
    "        cv_results = self._fit_grid_random_search(xgb_classifier, xgb_params)\n",
    "        return cv_results\n",
    "\n",
    "    def lgboost_classification(self):\n",
    "        \"\"\"\n",
    "        Light gradient boosting\n",
    "        \"\"\"\n",
    "        # parameters combinations\n",
    "        lgb_params = {\n",
    "            'num_leaves': sp_randint(6, 50),\n",
    "            'learning_rate': list(np.arange(0, 1.1, 0.4)),\n",
    "            'min_child_samples': sp_randint(100, 500),\n",
    "            'max_depth': list(range(5, 51, 5)),\n",
    "            'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n",
    "            'subsample': sp_uniform(loc=0.2, scale=0.8),\n",
    "            'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n",
    "            'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n",
    "            'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]\n",
    "        }\n",
    "\n",
    "        lgbm_classifier = lgb.LGBMClassifier(\n",
    "            random_state=314, silent=True, metric='None', n_jobs=4, n_estimators=5000)\n",
    "\n",
    "        cv_results = self._fit_grid_random_search(lgbm_classifier, lgb_params)\n",
    "        return cv_results\n",
    "\n",
    "    def et_classification(self):\n",
    "        \"\"\"\n",
    "        Extra tree Classification\n",
    "        \"\"\"\n",
    "        xt_clf = ExtraTreesClassifier(verbose=10,\n",
    "                                      random_state=123,\n",
    "                                      n_jobs=-1)\n",
    "\n",
    "        xt_params = {\n",
    "            'n_estimators': [int(x) for x in range(200, 2000, 200)],\n",
    "            'max_features': ['sqrt', 'auto', 'log2', None],\n",
    "            'max_depth': [int(x) for x in np.linspace(10, 110, num=11)],\n",
    "            'min_samples_leaf': sp_randint(1, 15),\n",
    "            'min_samples_split': sp_randint(2, 30),\n",
    "            'bootstrap': [True, False]}\n",
    "\n",
    "        cv_results = self._fit_grid_random_search(xt_clf, xt_params)\n",
    "        return cv_results\n",
    "\n",
    "    def grdient_boosting_classification(self):\n",
    "        \"\"\"\n",
    "        Gradient Boosting classifier\n",
    "        \"\"\"\n",
    "        lgb_params = {\n",
    "            'n_estimators': [int(x) for x in range(200, 2000, 200)],\n",
    "            'max_depth': [int(x) for x in np.linspace(10, 110, num=11)],\n",
    "            'learning_rate': [0.1, 0.001, 0.01]}\n",
    "\n",
    "        gbc_clf = GradientBoostingClassifier()\n",
    "        cv_results = self._fit_grid_random_search(gbc_clf, lgb_params)\n",
    "        return cv_results\n",
    "\n",
    "    def fitting_models(self):\n",
    "        \"\"\"\n",
    "        fitting all the models\n",
    "        \"\"\"\n",
    "        model_fitting_dict = {'dt': self.dt_classification(),\n",
    "                              'rf': self.rf_classification(),\n",
    "                              'ext': self.et_classification(),\n",
    "                              'gbc': self.grdient_boosting_classification(),\n",
    "                              'xgb': self.xgboost_classification(),\n",
    "                              'lgb': self.lgboost_classification(),\n",
    "                              'knn': self.knn_classification()\n",
    "                              }\n",
    "        return model_fitting_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5f8318d96463>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m            'Recall': 'recall'}\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtunning\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparameter_tuning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscoring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'N-BaIoT'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'random_search'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(r\"/gpfs/mariana/home/rkalak/xai_evaluation/project/Iot_Botnet_XAI/src/trained_models/\")\n",
    "scoring = {'accuracy': 'accuracy',\n",
    "           'AUC': 'roc_auc',\n",
    "           'F1': 'f1_micro',\n",
    "           'Precision': 'precision',\n",
    "           'Recall': 'recall'}\n",
    "\n",
    "tunning = parameter_tuning(X, y,scoring, 'N-BaIoT', 'random_search')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metric:{'accuracy': 'accuracy', 'AUC': 'roc_auc', 'F1': 'f1_micro', 'Precision': 'precision', 'Recall': 'recall'}\n",
      "<class 'dict'>\n",
      "Tuning Type:('random_search',)\n",
      "\n",
      "Classifier is XGBClassifier\n",
      "X variable: (8000, 115)\n",
      "y Variable:(8000,)\n",
      "Search Type:random_search\n",
      "\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV 1/5; 1/10] START colsample_bytree=0.948031113846764, learning_rate=0.0, max_depth=15, min_child_samples=258, min_child_weight=10000.0, num_leaves=43, reg_alpha=2, reg_lambda=0, subsample=0.25562105422078213\n",
      "[15:11:51] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:11:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5; 1/10] END colsample_bytree=0.948031113846764, learning_rate=0.0, max_depth=15, min_child_samples=258, min_child_weight=10000.0, num_leaves=43, reg_alpha=2, reg_lambda=0, subsample=0.25562105422078213; AUC: (train=0.500, test=0.500) F1: (train=0.503, test=0.486) Precision: (train=0.000, test=0.000) Recall: (train=0.000, test=0.000) accuracy: (train=0.503, test=0.486) total time=   0.3s\n",
      "[CV 2/5; 1/10] START colsample_bytree=0.948031113846764, learning_rate=0.0, max_depth=15, min_child_samples=258, min_child_weight=10000.0, num_leaves=43, reg_alpha=2, reg_lambda=0, subsample=0.25562105422078213\n",
      "[15:11:51] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:11:51] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 1/10] END colsample_bytree=0.948031113846764, learning_rate=0.0, max_depth=15, min_child_samples=258, min_child_weight=10000.0, num_leaves=43, reg_alpha=2, reg_lambda=0, subsample=0.25562105422078213; AUC: (train=0.500, test=0.500) F1: (train=0.497, test=0.510) Precision: (train=0.000, test=0.000) Recall: (train=0.000, test=0.000) accuracy: (train=0.497, test=0.510) total time=   0.1s\n",
      "[CV 3/5; 1/10] START colsample_bytree=0.948031113846764, learning_rate=0.0, max_depth=15, min_child_samples=258, min_child_weight=10000.0, num_leaves=43, reg_alpha=2, reg_lambda=0, subsample=0.25562105422078213\n",
      "[15:11:52] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:11:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5; 1/10] END colsample_bytree=0.948031113846764, learning_rate=0.0, max_depth=15, min_child_samples=258, min_child_weight=10000.0, num_leaves=43, reg_alpha=2, reg_lambda=0, subsample=0.25562105422078213; AUC: (train=0.500, test=0.500) F1: (train=0.499, test=0.504) Precision: (train=0.000, test=0.000) Recall: (train=0.000, test=0.000) accuracy: (train=0.499, test=0.504) total time=   0.1s\n",
      "[CV 4/5; 1/10] START colsample_bytree=0.948031113846764, learning_rate=0.0, max_depth=15, min_child_samples=258, min_child_weight=10000.0, num_leaves=43, reg_alpha=2, reg_lambda=0, subsample=0.25562105422078213\n",
      "[15:11:52] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:11:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 1/10] END colsample_bytree=0.948031113846764, learning_rate=0.0, max_depth=15, min_child_samples=258, min_child_weight=10000.0, num_leaves=43, reg_alpha=2, reg_lambda=0, subsample=0.25562105422078213; AUC: (train=0.500, test=0.500) F1: (train=0.501, test=0.497) Precision: (train=0.000, test=0.000) Recall: (train=0.000, test=0.000) accuracy: (train=0.501, test=0.497) total time=   0.1s\n",
      "[CV 5/5; 1/10] START colsample_bytree=0.948031113846764, learning_rate=0.0, max_depth=15, min_child_samples=258, min_child_weight=10000.0, num_leaves=43, reg_alpha=2, reg_lambda=0, subsample=0.25562105422078213\n",
      "[15:11:52] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:11:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 1/10] END colsample_bytree=0.948031113846764, learning_rate=0.0, max_depth=15, min_child_samples=258, min_child_weight=10000.0, num_leaves=43, reg_alpha=2, reg_lambda=0, subsample=0.25562105422078213; AUC: (train=0.500, test=0.500) F1: (train=0.499, test=0.502) Precision: (train=0.000, test=0.000) Recall: (train=0.000, test=0.000) accuracy: (train=0.499, test=0.502) total time=   0.1s\n",
      "[CV 1/5; 2/10] START colsample_bytree=0.9444137764884799, learning_rate=0.4, max_depth=45, min_child_samples=270, min_child_weight=1e-05, num_leaves=31, reg_alpha=50, reg_lambda=20, subsample=0.4469107185947923\n",
      "[15:11:52] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:11:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 2/10] END colsample_bytree=0.9444137764884799, learning_rate=0.4, max_depth=45, min_child_samples=270, min_child_weight=1e-05, num_leaves=31, reg_alpha=50, reg_lambda=20, subsample=0.4469107185947923; AUC: (train=1.000, test=1.000) F1: (train=0.998, test=0.998) Precision: (train=0.997, test=0.995) Recall: (train=1.000, test=1.000) accuracy: (train=0.998, test=0.998) total time=   0.2s\n",
      "[CV 2/5; 2/10] START colsample_bytree=0.9444137764884799, learning_rate=0.4, max_depth=45, min_child_samples=270, min_child_weight=1e-05, num_leaves=31, reg_alpha=50, reg_lambda=20, subsample=0.4469107185947923\n",
      "[15:11:52] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:11:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5; 2/10] END colsample_bytree=0.9444137764884799, learning_rate=0.4, max_depth=45, min_child_samples=270, min_child_weight=1e-05, num_leaves=31, reg_alpha=50, reg_lambda=20, subsample=0.4469107185947923; AUC: (train=1.000, test=1.000) F1: (train=0.998, test=0.998) Precision: (train=0.997, test=0.996) Recall: (train=1.000, test=1.000) accuracy: (train=0.998, test=0.998) total time=   0.2s\n",
      "[CV 3/5; 2/10] START colsample_bytree=0.9444137764884799, learning_rate=0.4, max_depth=45, min_child_samples=270, min_child_weight=1e-05, num_leaves=31, reg_alpha=50, reg_lambda=20, subsample=0.4469107185947923\n",
      "[15:11:52] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:11:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5; 2/10] END colsample_bytree=0.9444137764884799, learning_rate=0.4, max_depth=45, min_child_samples=270, min_child_weight=1e-05, num_leaves=31, reg_alpha=50, reg_lambda=20, subsample=0.4469107185947923; AUC: (train=1.000, test=1.000) F1: (train=0.998, test=0.995) Precision: (train=0.997, test=0.990) Recall: (train=1.000, test=1.000) accuracy: (train=0.998, test=0.995) total time=   0.2s\n",
      "[CV 4/5; 2/10] START colsample_bytree=0.9444137764884799, learning_rate=0.4, max_depth=45, min_child_samples=270, min_child_weight=1e-05, num_leaves=31, reg_alpha=50, reg_lambda=20, subsample=0.4469107185947923\n",
      "[15:11:52] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:11:52] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5; 2/10] END colsample_bytree=0.9444137764884799, learning_rate=0.4, max_depth=45, min_child_samples=270, min_child_weight=1e-05, num_leaves=31, reg_alpha=50, reg_lambda=20, subsample=0.4469107185947923; AUC: (train=1.000, test=1.000) F1: (train=0.998, test=0.999) Precision: (train=0.996, test=0.999) Recall: (train=1.000, test=1.000) accuracy: (train=0.998, test=0.999) total time=   0.2s\n",
      "[CV 5/5; 2/10] START colsample_bytree=0.9444137764884799, learning_rate=0.4, max_depth=45, min_child_samples=270, min_child_weight=1e-05, num_leaves=31, reg_alpha=50, reg_lambda=20, subsample=0.4469107185947923\n",
      "[15:11:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:11:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 2/10] END colsample_bytree=0.9444137764884799, learning_rate=0.4, max_depth=45, min_child_samples=270, min_child_weight=1e-05, num_leaves=31, reg_alpha=50, reg_lambda=20, subsample=0.4469107185947923; AUC: (train=1.000, test=1.000) F1: (train=0.998, test=0.998) Precision: (train=0.997, test=0.997) Recall: (train=1.000, test=0.999) accuracy: (train=0.998, test=0.998) total time=   0.2s\n",
      "[CV 1/5; 3/10] START colsample_bytree=0.7665782565593782, learning_rate=0.4, max_depth=20, min_child_samples=283, min_child_weight=1000.0, num_leaves=16, reg_alpha=0, reg_lambda=5, subsample=0.6150101711579531\n",
      "[15:11:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:11:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5; 3/10] END colsample_bytree=0.7665782565593782, learning_rate=0.4, max_depth=20, min_child_samples=283, min_child_weight=1000.0, num_leaves=16, reg_alpha=0, reg_lambda=5, subsample=0.6150101711579531; AUC: (train=0.500, test=0.500) F1: (train=0.503, test=0.486) Precision: (train=0.000, test=0.000) Recall: (train=0.000, test=0.000) accuracy: (train=0.503, test=0.486) total time=   0.1s\n",
      "[CV 2/5; 3/10] START colsample_bytree=0.7665782565593782, learning_rate=0.4, max_depth=20, min_child_samples=283, min_child_weight=1000.0, num_leaves=16, reg_alpha=0, reg_lambda=5, subsample=0.6150101711579531\n",
      "[15:11:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:11:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 3/10] END colsample_bytree=0.7665782565593782, learning_rate=0.4, max_depth=20, min_child_samples=283, min_child_weight=1000.0, num_leaves=16, reg_alpha=0, reg_lambda=5, subsample=0.6150101711579531; AUC: (train=0.500, test=0.500) F1: (train=0.497, test=0.510) Precision: (train=0.000, test=0.000) Recall: (train=0.000, test=0.000) accuracy: (train=0.497, test=0.510) total time=   0.1s\n",
      "[CV 3/5; 3/10] START colsample_bytree=0.7665782565593782, learning_rate=0.4, max_depth=20, min_child_samples=283, min_child_weight=1000.0, num_leaves=16, reg_alpha=0, reg_lambda=5, subsample=0.6150101711579531\n",
      "[15:11:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:11:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5; 3/10] END colsample_bytree=0.7665782565593782, learning_rate=0.4, max_depth=20, min_child_samples=283, min_child_weight=1000.0, num_leaves=16, reg_alpha=0, reg_lambda=5, subsample=0.6150101711579531; AUC: (train=0.500, test=0.500) F1: (train=0.499, test=0.504) Precision: (train=0.000, test=0.000) Recall: (train=0.000, test=0.000) accuracy: (train=0.499, test=0.504) total time=   0.1s\n",
      "[CV 4/5; 3/10] START colsample_bytree=0.7665782565593782, learning_rate=0.4, max_depth=20, min_child_samples=283, min_child_weight=1000.0, num_leaves=16, reg_alpha=0, reg_lambda=5, subsample=0.6150101711579531\n",
      "[15:11:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:11:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 3/10] END colsample_bytree=0.7665782565593782, learning_rate=0.4, max_depth=20, min_child_samples=283, min_child_weight=1000.0, num_leaves=16, reg_alpha=0, reg_lambda=5, subsample=0.6150101711579531; AUC: (train=0.500, test=0.500) F1: (train=0.501, test=0.497) Precision: (train=0.000, test=0.000) Recall: (train=0.000, test=0.000) accuracy: (train=0.501, test=0.497) total time=   0.1s\n",
      "[CV 5/5; 3/10] START colsample_bytree=0.7665782565593782, learning_rate=0.4, max_depth=20, min_child_samples=283, min_child_weight=1000.0, num_leaves=16, reg_alpha=0, reg_lambda=5, subsample=0.6150101711579531\n",
      "[15:11:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:11:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 3/10] END colsample_bytree=0.7665782565593782, learning_rate=0.4, max_depth=20, min_child_samples=283, min_child_weight=1000.0, num_leaves=16, reg_alpha=0, reg_lambda=5, subsample=0.6150101711579531; AUC: (train=0.500, test=0.500) F1: (train=0.499, test=0.502) Precision: (train=0.000, test=0.000) Recall: (train=0.000, test=0.000) accuracy: (train=0.499, test=0.502) total time=   0.1s\n",
      "[CV 1/5; 4/10] START colsample_bytree=0.4975643207961649, learning_rate=0.0, max_depth=15, min_child_samples=172, min_child_weight=0.1, num_leaves=39, reg_alpha=100, reg_lambda=5, subsample=0.7700835643113055\n",
      "[15:11:53] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:11:53] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 4/10] END colsample_bytree=0.4975643207961649, learning_rate=0.0, max_depth=15, min_child_samples=172, min_child_weight=0.1, num_leaves=39, reg_alpha=100, reg_lambda=5, subsample=0.7700835643113055; AUC: (train=0.500, test=0.500) F1: (train=0.503, test=0.486) Precision: (train=0.000, test=0.000) Recall: (train=0.000, test=0.000) accuracy: (train=0.503, test=0.486) total time=   0.3s\n",
      "[CV 2/5; 4/10] START colsample_bytree=0.4975643207961649, learning_rate=0.0, max_depth=15, min_child_samples=172, min_child_weight=0.1, num_leaves=39, reg_alpha=100, reg_lambda=5, subsample=0.7700835643113055\n",
      "[15:11:54] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:11:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 4/10] END colsample_bytree=0.4975643207961649, learning_rate=0.0, max_depth=15, min_child_samples=172, min_child_weight=0.1, num_leaves=39, reg_alpha=100, reg_lambda=5, subsample=0.7700835643113055; AUC: (train=0.500, test=0.500) F1: (train=0.497, test=0.510) Precision: (train=0.000, test=0.000) Recall: (train=0.000, test=0.000) accuracy: (train=0.497, test=0.510) total time=   0.5s\n",
      "[CV 3/5; 4/10] START colsample_bytree=0.4975643207961649, learning_rate=0.0, max_depth=15, min_child_samples=172, min_child_weight=0.1, num_leaves=39, reg_alpha=100, reg_lambda=5, subsample=0.7700835643113055\n",
      "[15:11:54] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:11:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 4/10] END colsample_bytree=0.4975643207961649, learning_rate=0.0, max_depth=15, min_child_samples=172, min_child_weight=0.1, num_leaves=39, reg_alpha=100, reg_lambda=5, subsample=0.7700835643113055; AUC: (train=0.500, test=0.500) F1: (train=0.499, test=0.504) Precision: (train=0.000, test=0.000) Recall: (train=0.000, test=0.000) accuracy: (train=0.499, test=0.504) total time=   0.3s\n",
      "[CV 4/5; 4/10] START colsample_bytree=0.4975643207961649, learning_rate=0.0, max_depth=15, min_child_samples=172, min_child_weight=0.1, num_leaves=39, reg_alpha=100, reg_lambda=5, subsample=0.7700835643113055\n",
      "[15:11:54] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:11:54] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 4/10] END colsample_bytree=0.4975643207961649, learning_rate=0.0, max_depth=15, min_child_samples=172, min_child_weight=0.1, num_leaves=39, reg_alpha=100, reg_lambda=5, subsample=0.7700835643113055; AUC: (train=0.500, test=0.500) F1: (train=0.501, test=0.497) Precision: (train=0.000, test=0.000) Recall: (train=0.000, test=0.000) accuracy: (train=0.501, test=0.497) total time=   0.3s\n",
      "[CV 5/5; 4/10] START colsample_bytree=0.4975643207961649, learning_rate=0.0, max_depth=15, min_child_samples=172, min_child_weight=0.1, num_leaves=39, reg_alpha=100, reg_lambda=5, subsample=0.7700835643113055\n",
      "[15:11:55] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:11:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 4/10] END colsample_bytree=0.4975643207961649, learning_rate=0.0, max_depth=15, min_child_samples=172, min_child_weight=0.1, num_leaves=39, reg_alpha=100, reg_lambda=5, subsample=0.7700835643113055; AUC: (train=0.500, test=0.500) F1: (train=0.499, test=0.502) Precision: (train=0.000, test=0.000) Recall: (train=0.000, test=0.000) accuracy: (train=0.499, test=0.502) total time=   0.3s\n",
      "[CV 1/5; 5/10] START colsample_bytree=0.7351046530333595, learning_rate=0.8, max_depth=45, min_child_samples=143, min_child_weight=1000.0, num_leaves=20, reg_alpha=0, reg_lambda=0.1, subsample=0.22634254006167903\n",
      "[15:11:55] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:11:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5; 5/10] END colsample_bytree=0.7351046530333595, learning_rate=0.8, max_depth=45, min_child_samples=143, min_child_weight=1000.0, num_leaves=20, reg_alpha=0, reg_lambda=0.1, subsample=0.22634254006167903; AUC: (train=0.500, test=0.500) F1: (train=0.503, test=0.486) Precision: (train=0.000, test=0.000) Recall: (train=0.000, test=0.000) accuracy: (train=0.503, test=0.486) total time=   0.1s\n",
      "[CV 2/5; 5/10] START colsample_bytree=0.7351046530333595, learning_rate=0.8, max_depth=45, min_child_samples=143, min_child_weight=1000.0, num_leaves=20, reg_alpha=0, reg_lambda=0.1, subsample=0.22634254006167903\n",
      "[15:11:55] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:11:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5; 5/10] END colsample_bytree=0.7351046530333595, learning_rate=0.8, max_depth=45, min_child_samples=143, min_child_weight=1000.0, num_leaves=20, reg_alpha=0, reg_lambda=0.1, subsample=0.22634254006167903; AUC: (train=0.500, test=0.500) F1: (train=0.497, test=0.510) Precision: (train=0.000, test=0.000) Recall: (train=0.000, test=0.000) accuracy: (train=0.497, test=0.510) total time=   0.1s\n",
      "[CV 3/5; 5/10] START colsample_bytree=0.7351046530333595, learning_rate=0.8, max_depth=45, min_child_samples=143, min_child_weight=1000.0, num_leaves=20, reg_alpha=0, reg_lambda=0.1, subsample=0.22634254006167903\n",
      "[15:11:55] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:11:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5; 5/10] END colsample_bytree=0.7351046530333595, learning_rate=0.8, max_depth=45, min_child_samples=143, min_child_weight=1000.0, num_leaves=20, reg_alpha=0, reg_lambda=0.1, subsample=0.22634254006167903; AUC: (train=0.500, test=0.500) F1: (train=0.499, test=0.504) Precision: (train=0.000, test=0.000) Recall: (train=0.000, test=0.000) accuracy: (train=0.499, test=0.504) total time=   0.1s\n",
      "[CV 4/5; 5/10] START colsample_bytree=0.7351046530333595, learning_rate=0.8, max_depth=45, min_child_samples=143, min_child_weight=1000.0, num_leaves=20, reg_alpha=0, reg_lambda=0.1, subsample=0.22634254006167903\n",
      "[15:11:55] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:11:55] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 4/5; 5/10] END colsample_bytree=0.7351046530333595, learning_rate=0.8, max_depth=45, min_child_samples=143, min_child_weight=1000.0, num_leaves=20, reg_alpha=0, reg_lambda=0.1, subsample=0.22634254006167903; AUC: (train=0.500, test=0.500) F1: (train=0.501, test=0.497) Precision: (train=0.000, test=0.000) Recall: (train=0.000, test=0.000) accuracy: (train=0.501, test=0.497) total time=   0.1s\n",
      "[CV 5/5; 5/10] START colsample_bytree=0.7351046530333595, learning_rate=0.8, max_depth=45, min_child_samples=143, min_child_weight=1000.0, num_leaves=20, reg_alpha=0, reg_lambda=0.1, subsample=0.22634254006167903\n",
      "[15:11:56] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:11:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 5/10] END colsample_bytree=0.7351046530333595, learning_rate=0.8, max_depth=45, min_child_samples=143, min_child_weight=1000.0, num_leaves=20, reg_alpha=0, reg_lambda=0.1, subsample=0.22634254006167903; AUC: (train=0.500, test=0.500) F1: (train=0.499, test=0.502) Precision: (train=0.000, test=0.000) Recall: (train=0.000, test=0.000) accuracy: (train=0.499, test=0.502) total time=   0.1s\n",
      "[CV 1/5; 6/10] START colsample_bytree=0.6487800392771721, learning_rate=0.0, max_depth=40, min_child_samples=389, min_child_weight=1000.0, num_leaves=9, reg_alpha=5, reg_lambda=50, subsample=0.9376523867859434\n",
      "[15:11:56] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:11:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 6/10] END colsample_bytree=0.6487800392771721, learning_rate=0.0, max_depth=40, min_child_samples=389, min_child_weight=1000.0, num_leaves=9, reg_alpha=5, reg_lambda=50, subsample=0.9376523867859434; AUC: (train=0.500, test=0.500) F1: (train=0.503, test=0.486) Precision: (train=0.000, test=0.000) Recall: (train=0.000, test=0.000) accuracy: (train=0.503, test=0.486) total time=   0.1s\n",
      "[CV 2/5; 6/10] START colsample_bytree=0.6487800392771721, learning_rate=0.0, max_depth=40, min_child_samples=389, min_child_weight=1000.0, num_leaves=9, reg_alpha=5, reg_lambda=50, subsample=0.9376523867859434\n",
      "[15:11:56] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:11:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5; 6/10] END colsample_bytree=0.6487800392771721, learning_rate=0.0, max_depth=40, min_child_samples=389, min_child_weight=1000.0, num_leaves=9, reg_alpha=5, reg_lambda=50, subsample=0.9376523867859434; AUC: (train=0.500, test=0.500) F1: (train=0.497, test=0.510) Precision: (train=0.000, test=0.000) Recall: (train=0.000, test=0.000) accuracy: (train=0.497, test=0.510) total time=   0.1s\n",
      "[CV 3/5; 6/10] START colsample_bytree=0.6487800392771721, learning_rate=0.0, max_depth=40, min_child_samples=389, min_child_weight=1000.0, num_leaves=9, reg_alpha=5, reg_lambda=50, subsample=0.9376523867859434\n",
      "[15:11:56] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:11:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 6/10] END colsample_bytree=0.6487800392771721, learning_rate=0.0, max_depth=40, min_child_samples=389, min_child_weight=1000.0, num_leaves=9, reg_alpha=5, reg_lambda=50, subsample=0.9376523867859434; AUC: (train=0.500, test=0.500) F1: (train=0.499, test=0.504) Precision: (train=0.000, test=0.000) Recall: (train=0.000, test=0.000) accuracy: (train=0.499, test=0.504) total time=   0.1s\n",
      "[CV 4/5; 6/10] START colsample_bytree=0.6487800392771721, learning_rate=0.0, max_depth=40, min_child_samples=389, min_child_weight=1000.0, num_leaves=9, reg_alpha=5, reg_lambda=50, subsample=0.9376523867859434\n",
      "[15:11:56] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:11:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5; 6/10] END colsample_bytree=0.6487800392771721, learning_rate=0.0, max_depth=40, min_child_samples=389, min_child_weight=1000.0, num_leaves=9, reg_alpha=5, reg_lambda=50, subsample=0.9376523867859434; AUC: (train=0.500, test=0.500) F1: (train=0.501, test=0.497) Precision: (train=0.000, test=0.000) Recall: (train=0.000, test=0.000) accuracy: (train=0.501, test=0.497) total time=   0.1s\n",
      "[CV 5/5; 6/10] START colsample_bytree=0.6487800392771721, learning_rate=0.0, max_depth=40, min_child_samples=389, min_child_weight=1000.0, num_leaves=9, reg_alpha=5, reg_lambda=50, subsample=0.9376523867859434\n",
      "[15:11:56] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:11:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 6/10] END colsample_bytree=0.6487800392771721, learning_rate=0.0, max_depth=40, min_child_samples=389, min_child_weight=1000.0, num_leaves=9, reg_alpha=5, reg_lambda=50, subsample=0.9376523867859434; AUC: (train=0.500, test=0.500) F1: (train=0.499, test=0.502) Precision: (train=0.000, test=0.000) Recall: (train=0.000, test=0.000) accuracy: (train=0.499, test=0.502) total time=   0.2s\n",
      "[CV 1/5; 7/10] START colsample_bytree=0.48993496423153826, learning_rate=0.4, max_depth=20, min_child_samples=342, min_child_weight=1, num_leaves=7, reg_alpha=0.1, reg_lambda=1, subsample=0.5189426197819126\n",
      "[15:11:56] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:11:56] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 7/10] END colsample_bytree=0.48993496423153826, learning_rate=0.4, max_depth=20, min_child_samples=342, min_child_weight=1, num_leaves=7, reg_alpha=0.1, reg_lambda=1, subsample=0.5189426197819126; AUC: (train=1.000, test=1.000) F1: (train=1.000, test=1.000) Precision: (train=1.000, test=1.000) Recall: (train=1.000, test=1.000) accuracy: (train=1.000, test=1.000) total time=   0.1s\n",
      "[CV 2/5; 7/10] START colsample_bytree=0.48993496423153826, learning_rate=0.4, max_depth=20, min_child_samples=342, min_child_weight=1, num_leaves=7, reg_alpha=0.1, reg_lambda=1, subsample=0.5189426197819126\n",
      "[15:11:57] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:11:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5; 7/10] END colsample_bytree=0.48993496423153826, learning_rate=0.4, max_depth=20, min_child_samples=342, min_child_weight=1, num_leaves=7, reg_alpha=0.1, reg_lambda=1, subsample=0.5189426197819126; AUC: (train=1.000, test=1.000) F1: (train=1.000, test=0.999) Precision: (train=1.000, test=0.999) Recall: (train=1.000, test=1.000) accuracy: (train=1.000, test=0.999) total time=   0.1s\n",
      "[CV 3/5; 7/10] START colsample_bytree=0.48993496423153826, learning_rate=0.4, max_depth=20, min_child_samples=342, min_child_weight=1, num_leaves=7, reg_alpha=0.1, reg_lambda=1, subsample=0.5189426197819126\n",
      "[15:11:57] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:11:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5; 7/10] END colsample_bytree=0.48993496423153826, learning_rate=0.4, max_depth=20, min_child_samples=342, min_child_weight=1, num_leaves=7, reg_alpha=0.1, reg_lambda=1, subsample=0.5189426197819126; AUC: (train=1.000, test=1.000) F1: (train=1.000, test=0.999) Precision: (train=1.000, test=0.999) Recall: (train=1.000, test=1.000) accuracy: (train=1.000, test=0.999) total time=   0.1s\n",
      "[CV 4/5; 7/10] START colsample_bytree=0.48993496423153826, learning_rate=0.4, max_depth=20, min_child_samples=342, min_child_weight=1, num_leaves=7, reg_alpha=0.1, reg_lambda=1, subsample=0.5189426197819126\n",
      "[15:11:57] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:11:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5; 7/10] END colsample_bytree=0.48993496423153826, learning_rate=0.4, max_depth=20, min_child_samples=342, min_child_weight=1, num_leaves=7, reg_alpha=0.1, reg_lambda=1, subsample=0.5189426197819126; AUC: (train=1.000, test=1.000) F1: (train=1.000, test=1.000) Precision: (train=1.000, test=1.000) Recall: (train=1.000, test=1.000) accuracy: (train=1.000, test=1.000) total time=   0.1s\n",
      "[CV 5/5; 7/10] START colsample_bytree=0.48993496423153826, learning_rate=0.4, max_depth=20, min_child_samples=342, min_child_weight=1, num_leaves=7, reg_alpha=0.1, reg_lambda=1, subsample=0.5189426197819126\n",
      "[15:11:57] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:11:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 7/10] END colsample_bytree=0.48993496423153826, learning_rate=0.4, max_depth=20, min_child_samples=342, min_child_weight=1, num_leaves=7, reg_alpha=0.1, reg_lambda=1, subsample=0.5189426197819126; AUC: (train=1.000, test=1.000) F1: (train=1.000, test=0.999) Precision: (train=1.000, test=1.000) Recall: (train=1.000, test=0.999) accuracy: (train=1.000, test=0.999) total time=   0.1s\n",
      "[CV 1/5; 8/10] START colsample_bytree=0.9562770851104112, learning_rate=0.4, max_depth=35, min_child_samples=132, min_child_weight=1e-05, num_leaves=47, reg_alpha=1, reg_lambda=0, subsample=0.8166615943825004\n",
      "[15:11:57] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:11:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5; 8/10] END colsample_bytree=0.9562770851104112, learning_rate=0.4, max_depth=35, min_child_samples=132, min_child_weight=1e-05, num_leaves=47, reg_alpha=1, reg_lambda=0, subsample=0.8166615943825004; AUC: (train=1.000, test=1.000) F1: (train=1.000, test=0.999) Precision: (train=1.000, test=0.999) Recall: (train=1.000, test=1.000) accuracy: (train=1.000, test=0.999) total time=   0.2s\n",
      "[CV 2/5; 8/10] START colsample_bytree=0.9562770851104112, learning_rate=0.4, max_depth=35, min_child_samples=132, min_child_weight=1e-05, num_leaves=47, reg_alpha=1, reg_lambda=0, subsample=0.8166615943825004\n",
      "[15:11:57] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:11:57] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5; 8/10] END colsample_bytree=0.9562770851104112, learning_rate=0.4, max_depth=35, min_child_samples=132, min_child_weight=1e-05, num_leaves=47, reg_alpha=1, reg_lambda=0, subsample=0.8166615943825004; AUC: (train=1.000, test=1.000) F1: (train=1.000, test=0.999) Precision: (train=1.000, test=0.999) Recall: (train=1.000, test=1.000) accuracy: (train=1.000, test=0.999) total time=   0.2s\n",
      "[CV 3/5; 8/10] START colsample_bytree=0.9562770851104112, learning_rate=0.4, max_depth=35, min_child_samples=132, min_child_weight=1e-05, num_leaves=47, reg_alpha=1, reg_lambda=0, subsample=0.8166615943825004\n",
      "[15:11:58] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:11:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 3/5; 8/10] END colsample_bytree=0.9562770851104112, learning_rate=0.4, max_depth=35, min_child_samples=132, min_child_weight=1e-05, num_leaves=47, reg_alpha=1, reg_lambda=0, subsample=0.8166615943825004; AUC: (train=1.000, test=1.000) F1: (train=1.000, test=0.999) Precision: (train=1.000, test=0.999) Recall: (train=1.000, test=1.000) accuracy: (train=1.000, test=0.999) total time=   0.2s\n",
      "[CV 4/5; 8/10] START colsample_bytree=0.9562770851104112, learning_rate=0.4, max_depth=35, min_child_samples=132, min_child_weight=1e-05, num_leaves=47, reg_alpha=1, reg_lambda=0, subsample=0.8166615943825004\n",
      "[15:11:58] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:11:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5; 8/10] END colsample_bytree=0.9562770851104112, learning_rate=0.4, max_depth=35, min_child_samples=132, min_child_weight=1e-05, num_leaves=47, reg_alpha=1, reg_lambda=0, subsample=0.8166615943825004; AUC: (train=1.000, test=1.000) F1: (train=1.000, test=0.999) Precision: (train=1.000, test=1.000) Recall: (train=1.000, test=0.999) accuracy: (train=1.000, test=0.999) total time=   0.2s\n",
      "[CV 5/5; 8/10] START colsample_bytree=0.9562770851104112, learning_rate=0.4, max_depth=35, min_child_samples=132, min_child_weight=1e-05, num_leaves=47, reg_alpha=1, reg_lambda=0, subsample=0.8166615943825004\n",
      "[15:11:58] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:11:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 8/10] END colsample_bytree=0.9562770851104112, learning_rate=0.4, max_depth=35, min_child_samples=132, min_child_weight=1e-05, num_leaves=47, reg_alpha=1, reg_lambda=0, subsample=0.8166615943825004; AUC: (train=1.000, test=1.000) F1: (train=1.000, test=0.999) Precision: (train=1.000, test=1.000) Recall: (train=1.000, test=0.999) accuracy: (train=1.000, test=0.999) total time=   0.2s\n",
      "[CV 1/5; 9/10] START colsample_bytree=0.6519072208581196, learning_rate=0.8, max_depth=30, min_child_samples=252, min_child_weight=0.001, num_leaves=7, reg_alpha=10, reg_lambda=20, subsample=0.2636564011555459\n",
      "[15:11:58] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:11:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 1/5; 9/10] END colsample_bytree=0.6519072208581196, learning_rate=0.8, max_depth=30, min_child_samples=252, min_child_weight=0.001, num_leaves=7, reg_alpha=10, reg_lambda=20, subsample=0.2636564011555459; AUC: (train=1.000, test=1.000) F1: (train=0.999, test=0.999) Precision: (train=0.999, test=0.999) Recall: (train=1.000, test=0.999) accuracy: (train=0.999, test=0.999) total time=   0.1s\n",
      "[CV 2/5; 9/10] START colsample_bytree=0.6519072208581196, learning_rate=0.8, max_depth=30, min_child_samples=252, min_child_weight=0.001, num_leaves=7, reg_alpha=10, reg_lambda=20, subsample=0.2636564011555459\n",
      "[15:11:58] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:11:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5; 9/10] END colsample_bytree=0.6519072208581196, learning_rate=0.8, max_depth=30, min_child_samples=252, min_child_weight=0.001, num_leaves=7, reg_alpha=10, reg_lambda=20, subsample=0.2636564011555459; AUC: (train=1.000, test=1.000) F1: (train=0.999, test=0.999) Precision: (train=0.998, test=0.999) Recall: (train=1.000, test=1.000) accuracy: (train=0.999, test=0.999) total time=   0.1s\n",
      "[CV 3/5; 9/10] START colsample_bytree=0.6519072208581196, learning_rate=0.8, max_depth=30, min_child_samples=252, min_child_weight=0.001, num_leaves=7, reg_alpha=10, reg_lambda=20, subsample=0.2636564011555459\n",
      "[15:11:58] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:11:58] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5; 9/10] END colsample_bytree=0.6519072208581196, learning_rate=0.8, max_depth=30, min_child_samples=252, min_child_weight=0.001, num_leaves=7, reg_alpha=10, reg_lambda=20, subsample=0.2636564011555459; AUC: (train=1.000, test=1.000) F1: (train=0.999, test=0.999) Precision: (train=0.998, test=0.997) Recall: (train=1.000, test=1.000) accuracy: (train=0.999, test=0.999) total time=   0.1s\n",
      "[CV 4/5; 9/10] START colsample_bytree=0.6519072208581196, learning_rate=0.8, max_depth=30, min_child_samples=252, min_child_weight=0.001, num_leaves=7, reg_alpha=10, reg_lambda=20, subsample=0.2636564011555459\n",
      "[15:11:59] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:11:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5; 9/10] END colsample_bytree=0.6519072208581196, learning_rate=0.8, max_depth=30, min_child_samples=252, min_child_weight=0.001, num_leaves=7, reg_alpha=10, reg_lambda=20, subsample=0.2636564011555459; AUC: (train=1.000, test=1.000) F1: (train=0.998, test=0.999) Precision: (train=0.998, test=0.999) Recall: (train=0.999, test=1.000) accuracy: (train=0.998, test=0.999) total time=   0.1s\n",
      "[CV 5/5; 9/10] START colsample_bytree=0.6519072208581196, learning_rate=0.8, max_depth=30, min_child_samples=252, min_child_weight=0.001, num_leaves=7, reg_alpha=10, reg_lambda=20, subsample=0.2636564011555459\n",
      "[15:11:59] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:11:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 9/10] END colsample_bytree=0.6519072208581196, learning_rate=0.8, max_depth=30, min_child_samples=252, min_child_weight=0.001, num_leaves=7, reg_alpha=10, reg_lambda=20, subsample=0.2636564011555459; AUC: (train=1.000, test=1.000) F1: (train=0.999, test=0.999) Precision: (train=0.998, test=1.000) Recall: (train=1.000, test=0.997) accuracy: (train=0.999, test=0.999) total time=   0.1s\n",
      "[CV 1/5; 10/10] START colsample_bytree=0.48815639187225207, learning_rate=0.4, max_depth=20, min_child_samples=143, min_child_weight=0.001, num_leaves=39, reg_alpha=100, reg_lambda=10, subsample=0.6848484029926902\n",
      "[15:11:59] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:11:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 1/5; 10/10] END colsample_bytree=0.48815639187225207, learning_rate=0.4, max_depth=20, min_child_samples=143, min_child_weight=0.001, num_leaves=39, reg_alpha=100, reg_lambda=10, subsample=0.6848484029926902; AUC: (train=1.000, test=1.000) F1: (train=0.998, test=0.998) Precision: (train=0.997, test=0.995) Recall: (train=1.000, test=1.000) accuracy: (train=0.998, test=0.998) total time=   0.2s\n",
      "[CV 2/5; 10/10] START colsample_bytree=0.48815639187225207, learning_rate=0.4, max_depth=20, min_child_samples=143, min_child_weight=0.001, num_leaves=39, reg_alpha=100, reg_lambda=10, subsample=0.6848484029926902\n",
      "[15:11:59] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:11:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 2/5; 10/10] END colsample_bytree=0.48815639187225207, learning_rate=0.4, max_depth=20, min_child_samples=143, min_child_weight=0.001, num_leaves=39, reg_alpha=100, reg_lambda=10, subsample=0.6848484029926902; AUC: (train=1.000, test=1.000) F1: (train=0.999, test=0.999) Precision: (train=0.998, test=0.997) Recall: (train=1.000, test=1.000) accuracy: (train=0.999, test=0.999) total time=   0.1s\n",
      "[CV 3/5; 10/10] START colsample_bytree=0.48815639187225207, learning_rate=0.4, max_depth=20, min_child_samples=143, min_child_weight=0.001, num_leaves=39, reg_alpha=100, reg_lambda=10, subsample=0.6848484029926902\n",
      "[15:11:59] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:11:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 3/5; 10/10] END colsample_bytree=0.48815639187225207, learning_rate=0.4, max_depth=20, min_child_samples=143, min_child_weight=0.001, num_leaves=39, reg_alpha=100, reg_lambda=10, subsample=0.6848484029926902; AUC: (train=1.000, test=1.000) F1: (train=0.999, test=0.999) Precision: (train=0.998, test=0.997) Recall: (train=1.000, test=1.000) accuracy: (train=0.999, test=0.999) total time=   0.1s\n",
      "[CV 4/5; 10/10] START colsample_bytree=0.48815639187225207, learning_rate=0.4, max_depth=20, min_child_samples=143, min_child_weight=0.001, num_leaves=39, reg_alpha=100, reg_lambda=10, subsample=0.6848484029926902\n",
      "[15:11:59] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:11:59] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 4/5; 10/10] END colsample_bytree=0.48815639187225207, learning_rate=0.4, max_depth=20, min_child_samples=143, min_child_weight=0.001, num_leaves=39, reg_alpha=100, reg_lambda=10, subsample=0.6848484029926902; AUC: (train=1.000, test=1.000) F1: (train=0.999, test=0.999) Precision: (train=0.998, test=0.999) Recall: (train=1.000, test=1.000) accuracy: (train=0.999, test=0.999) total time=   0.1s\n",
      "[CV 5/5; 10/10] START colsample_bytree=0.48815639187225207, learning_rate=0.4, max_depth=20, min_child_samples=143, min_child_weight=0.001, num_leaves=39, reg_alpha=100, reg_lambda=10, subsample=0.6848484029926902\n",
      "[15:12:00] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:12:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[CV 5/5; 10/10] END colsample_bytree=0.48815639187225207, learning_rate=0.4, max_depth=20, min_child_samples=143, min_child_weight=0.001, num_leaves=39, reg_alpha=100, reg_lambda=10, subsample=0.6848484029926902; AUC: (train=1.000, test=1.000) F1: (train=0.998, test=0.999) Precision: (train=0.997, test=0.999) Recall: (train=1.000, test=0.999) accuracy: (train=0.998, test=0.999) total time=   0.1s\n",
      "[15:12:00] WARNING: ../src/learner.cc:576: \n",
      "Parameters: { \"min_child_samples\", \"num_leaves\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[15:12:00] WARNING: ../src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "================================================================================\n",
      "Best parameters:{'colsample_bytree': 0.48993496423153826, 'learning_rate': 0.4, 'max_depth': 20, 'min_child_samples': 342, 'min_child_weight': 1, 'num_leaves': 7, 'reg_alpha': 0.1, 'reg_lambda': 1, 'subsample': 0.5189426197819126}\n",
      "Best Estimator:XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "              colsample_bynode=1, colsample_bytree=0.48993496423153826,\n",
      "              enable_categorical=False, gamma=0, gpu_id=-1,\n",
      "              importance_type=None, interaction_constraints='',\n",
      "              learning_rate=0.4, max_delta_step=0, max_depth=20,\n",
      "              min_child_samples=342, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=100, n_jobs=32,\n",
      "              num_leaves=7, num_parallel_tree=1, predictor='auto',\n",
      "              random_state=100, reg_alpha=0.1, reg_lambda=1, scale_pos_weight=1,\n",
      "              subsample=0.5189426197819126, tree_method='exact',\n",
      "              use_label_encoder=False, validate_parameters=1, ...)\n",
      "Best score:0.9999968749057582\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "res = tunning.xgboost_classification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = joblib.load(fl_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 0.1.36ubuntu1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 0.23ubuntu1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "Name: scikit-learn\n",
      "Version: 0.23.2\n",
      "Summary: A set of python modules for machine learning and data mining\n",
      "Home-page: http://scikit-learn.org\n",
      "Author: None\n",
      "Author-email: None\n",
      "License: new BSD\n",
      "Location: /gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages\n",
      "Requires: numpy, scipy, joblib, threadpoolctl\n",
      "Required-by: PyImpetus, lightgbm, pyLDAvis, pycaret, bamboolib, yellowbrick, ppscore, pyod, skrebate, shap, mlxtend, kmodes, imbalanced-learn, pynndescent, umap-learn, scikit-plot, Boruta, lime, aix360, cvplot, sklearn\n"
     ]
    }
   ],
   "source": [
    "!pip show scikit-learn  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 0.1.36ubuntu1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "/gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages/pkg_resources/__init__.py:123: PkgResourcesDeprecationWarning: 0.23ubuntu1 is an invalid version and will not be supported in a future release\n",
      "  warnings.warn(\n",
      "Package                           Version                Location                                                    \n",
      "--------------------------------- ---------------------- ------------------------------------------------------------\n",
      "about-time                        3.1.1                  \n",
      "absl-py                           1.0.0                  \n",
      "aix360                            0.1.0                  \n",
      "alembic                           1.8.1                  \n",
      "alive-progress                    2.4.1                  \n",
      "analytics-python                  1.2.9                  \n",
      "anyio                             3.4.0                  \n",
      "appdirs                           1.4.3                  \n",
      "argon2-cffi                       21.1.0                 \n",
      "asgiref                           3.5.2                  \n",
      "astor                             0.8.1                  \n",
      "astunparse                        1.6.3                  \n",
      "attrs                             21.4.0                 \n",
      "Automat                           0.8.0                  \n",
      "autopep8                          1.5                    \n",
      "Babel                             2.9.1                  \n",
      "backcall                          0.1.0                  \n",
      "backports.zoneinfo                0.2.1                  \n",
      "bamboolib                         1.29.1                 \n",
      "bcrypt                            3.1.7                  \n",
      "black                             21.12b1.dev25+g8a84beb \n",
      "bleach                            3.1.1                  \n",
      "blinker                           1.4                    \n",
      "blis                              0.7.8                  \n",
      "Boruta                            0.3                    \n",
      "Bottleneck                        1.3.2                  \n",
      "cachetools                        4.2.4                  \n",
      "catalogue                         1.0.0                  \n",
      "ceph                              1.0.0                  \n",
      "cephfs                            2.0.0                  \n",
      "certifi                           2019.11.28             \n",
      "cffi                              1.15.0                 \n",
      "cfgv                              3.3.1                  \n",
      "chardet                           3.0.4                  \n",
      "click                             8.0.3                  \n",
      "cloud-init                        22.3.4                 \n",
      "cloudpickle                       2.0.0                  \n",
      "colorama                          0.4.3                  \n",
      "colorlover                        0.3.0                  \n",
      "command-not-found                 0.3                    \n",
      "composition                       8!6.75309              \n",
      "configobj                         5.0.6                  \n",
      "constantly                        15.1.0                 \n",
      "cryptography                      2.8                    \n",
      "cufflinks                         0.17.3                 \n",
      "cupshelpers                       1.0                    \n",
      "cvplot                            0.0.2                  \n",
      "cvxopt                            1.3.0                  \n",
      "cvxpy                             1.2.1                  \n",
      "cycler                            0.11.0                 \n",
      "cymem                             2.0.6                  \n",
      "data                              0.4                    \n",
      "databricks-cli                    0.17.3                 \n",
      "dbus-python                       1.2.16                 \n",
      "decorator                         4.4.2                  \n",
      "defer                             1.0.6                  \n",
      "defusedxml                        0.6.0                  \n",
      "distlib                           0.3.0                  \n",
      "distro                            1.4.0                  \n",
      "distro-info                       0.23ubuntu1            \n",
      "Django                            4.0.6                  \n",
      "docker                            6.0.0                  \n",
      "docutils                          0.16                   \n",
      "ecos                              2.0.10                 \n",
      "entrypoints                       0.3                    \n",
      "et-xmlfile                        1.1.0                  \n",
      "evdev                             1.4.0                  \n",
      "fail2ban                          0.11.1                 \n",
      "filelock                          3.0.12                 \n",
      "flake8                            3.9.2                  \n",
      "Flask                             2.2.2                  \n",
      "flatbuffers                       2.0                    \n",
      "fonttools                         4.33.3                 \n",
      "forbiddenfruit                    0.1.4                  \n",
      "freetype-py                       2.2.0                  \n",
      "funcsigs                          1.0.2                  \n",
      "funcy                             1.17                   \n",
      "future                            0.18.2                 \n",
      "gast                              0.4.0                  \n",
      "GDAL                              3.0.4                  \n",
      "gensim                            3.8.3                  \n",
      "gitdb                             4.0.9                  \n",
      "gitdb2                            2.0.6                  \n",
      "githubcommit                      0.1.0                  \n",
      "GitPython                         3.1.27                 \n",
      "google-api-core                   2.7.1                  \n",
      "google-api-python-client          2.41.0                 \n",
      "google-auth                       2.3.3                  \n",
      "google-auth-httplib2              0.1.0                  \n",
      "google-auth-oauthlib              0.4.6                  \n",
      "google-pasta                      0.2.0                  \n",
      "googleapis-common-protos          1.55.0                 \n",
      "grapheme                          0.6.0                  \n",
      "graphviz                          0.19.1                 \n",
      "greenlet                          1.1.3                  \n",
      "grpcio                            1.43.0                 \n",
      "gunicorn                          20.1.0                 \n",
      "h5py                              3.6.0                  \n",
      "html5lib                          1.0.1                  \n",
      "htmlmin                           0.1.12                 \n",
      "httplib2                          0.20.4                 \n",
      "hyperlink                         19.0.0                 \n",
      "hyperopt                          0.2.7                  \n",
      "identify                          2.5.1                  \n",
      "idna                              2.8                    \n",
      "image                             1.5.33                 \n",
      "ImageHash                         4.3.1                  \n",
      "imageio                           2.13.1                 \n",
      "imbalanced-learn                  0.7.0                  \n",
      "imblearn                          0.0                    \n",
      "importlib-metadata                4.10.0                 \n",
      "importlib-resources               5.9.0                  \n",
      "incremental                       16.10.1                \n",
      "ipykernel                         5.2.0                  \n",
      "ipyslickgrid                      0.0.3                  \n",
      "ipython                           7.30.1                 \n",
      "ipython-genutils                  0.2.0                  \n",
      "ipywidgets                        7.6.5                  \n",
      "isort                             5.10.1                 \n",
      "itsdangerous                      2.1.2                  \n",
      "jedi                              0.18.1                 \n",
      "Jinja2                            2.10.1                 \n",
      "joblib                            1.1.0                  \n",
      "json5                             0.9.6                  \n",
      "jsonpatch                         1.22                   \n",
      "jsonpointer                       2.0                    \n",
      "jsonschema                        3.2.0                  \n",
      "jupyter-client                    7.1.0                  \n",
      "jupyter-console                   6.0.0                  \n",
      "jupyter-contrib-core              0.3.3                  \n",
      "jupyter-contrib-nbextensions      0.5.1                  /gpfs/mariana/home/rkalak/.local/lib/python3.8/site-packages\n",
      "jupyter-core                      4.6.3                  \n",
      "jupyter-highlight-selected-word   0.2.0                  \n",
      "jupyter-latex-envs                1.4.6                  \n",
      "jupyter-nbextensions-configurator 0.4.1                  \n",
      "jupyter-server                    1.12.1                 \n",
      "jupyterlab                        2.3.2                  \n",
      "jupyterlab-pygments               0.1.2                  \n",
      "jupyterlab-server                 1.2.0                  \n",
      "jupyterlab-slurm                  3.0.1                  \n",
      "jupyterlab-widgets                1.0.2                  \n",
      "jupyterthemes                     0.20.0                 \n",
      "keras                             2.7.0                  \n",
      "Keras-Preprocessing               1.1.2                  \n",
      "keyring                           18.0.1                 \n",
      "kiwisolver                        1.4.4                  \n",
      "kmodes                            0.12.2                 \n",
      "language-selector                 0.1                    \n",
      "latex                             0.7.0                  \n",
      "launchpadlib                      1.10.13                \n",
      "lazr.restfulclient                0.14.2                 \n",
      "lazr.uri                          1.0.3                  \n",
      "lesscpy                           0.15.0                 \n",
      "libclang                          12.0.0                 \n",
      "lightgbm                          3.3.1                  \n",
      "lime                              0.2.0.1                \n",
      "llvmlite                          0.39.1                 \n",
      "lockfile                          0.12.2                 \n",
      "lxml                              4.6.4                  \n",
      "macaroonbakery                    1.3.1                  \n",
      "Mako                              1.2.3                  \n",
      "Markdown                          3.3.6                  \n",
      "MarkupSafe                        1.1.0                  \n",
      "matplotlib                        3.5.2                  \n",
      "matplotlib-inline                 0.1.3                  \n",
      "mccabe                            0.6.1                  \n",
      "missingno                         0.5.1                  \n",
      "mistune                           0.8.4                  \n",
      "mitoinstaller                     0.0.94                 \n",
      "mitosheet                         0.1.368                \n",
      "mlflow                            1.29.0                 \n",
      "mlxtend                           0.19.0                 \n",
      "more-itertools                    8.10.0                 \n",
      "mpmath                            1.2.1                  \n",
      "multimethod                       1.8                    \n",
      "murmurhash                        1.0.8                  \n",
      "mypy-extensions                   0.4.3                  \n",
      "mysqlclient                       1.4.4                  \n",
      "namedtensor                       0.0.2                  \n",
      "nbclassic                         0.3.4                  \n",
      "nbclient                          0.5.9                  \n",
      "nbconvert                         6.4.0                  \n",
      "nbformat                          5.0.4                  \n",
      "nest-asyncio                      1.5.4                  \n",
      "netifaces                         0.10.4                 \n",
      "networkx                          2.6.3                  \n",
      "nftables                          0.1                    \n",
      "nltk                              3.7                    \n",
      "nodeenv                           1.6.0                  \n",
      "notebook                          6.0.3                  \n",
      "numba                             0.56.3                 \n",
      "numexpr                           2.8.3                  \n",
      "numpy                             1.22.4                 \n",
      "oauth2client                      4.1.3                  \n",
      "oauthlib                          3.1.0                  \n",
      "openpyxl                          3.0.9                  \n",
      "opt-einsum                        3.3.0                  \n",
      "osqp                              0.6.2.post5            \n",
      "packaging                         21.3                   \n",
      "pandas                            1.3.5                  \n",
      "pandas-profiling                  3.3.0                  \n",
      "pandasgui                         0.2.13                 \n",
      "pandocfilters                     1.4.2                  \n",
      "paramiko                          2.6.0                  \n",
      "parso                             0.8.3                  \n",
      "pathspec                          0.9.0                  \n",
      "patsy                             0.5.2                  \n",
      "pep8                              1.7.1                  \n",
      "pexpect                           4.6.0                  \n",
      "phik                              0.12.2                 \n",
      "pickleshare                       0.7.5                  \n",
      "Pillow                            9.2.0                  \n",
      "pip                               20.0.2                 \n",
      "pivottablejs                      0.9.0                  \n",
      "plac                              1.1.3                  \n",
      "platformdirs                      2.4.1                  \n",
      "plotly                            5.0.0                  \n",
      "ply                               3.11                   \n",
      "ppscore                           1.2.0                  \n",
      "pre-commit                        2.19.0                 \n",
      "preshed                           3.0.7                  \n",
      "pretty-confusion-matrix           0.1.1                  \n",
      "prettytable                       0.7.2                  \n",
      "progress                          1.6                    \n",
      "progressbar                       2.5                    \n",
      "progressbar2                      4.0.0                  \n",
      "prometheus-client                 0.7.1                  \n",
      "prometheus-flask-exporter         0.20.3                 \n",
      "prompt-toolkit                    2.0.10                 \n",
      "protobuf                          3.19.1                 \n",
      "psutil                            5.8.0                  \n",
      "ptyprocess                        0.7.0                  \n",
      "py4j                              0.10.9.5               \n",
      "pyarrow                           6.0.1                  \n",
      "pyasn1                            0.4.2                  \n",
      "pyasn1-modules                    0.2.1                  \n",
      "pycairo                           1.16.2                 \n",
      "pycaret                           2.3.10                 \n",
      "pycodestyle                       2.7.0                  \n",
      "pycparser                         2.19                   \n",
      "pycups                            1.9.73                 \n",
      "pydantic                          1.9.2                  \n",
      "pydot                             1.4.1                  \n",
      "pyflakes                          2.3.1                  \n",
      "Pygments                          2.10.0                 \n",
      "PyGObject                         3.36.0                 \n",
      "PyHamcrest                        1.9.0                  \n",
      "PyImpetus                         4.0.1                  \n",
      "pyinotify                         0.9.6                  \n",
      "PyJWT                             1.7.1                  \n",
      "pyLDAvis                          3.3.1                  \n",
      "pylibacl                          0.5.4                  \n",
      "pymacaroons                       0.13.0                 \n",
      "PyNaCl                            1.3.0                  \n",
      "pynndescent                       0.5.7                  \n",
      "pynput                            1.7.6                  \n",
      "pyod                              1.0.5                  \n",
      "pyOpenSSL                         19.0.0                 \n",
      "pyparsing                         2.4.6                  \n",
      "PyQt5                             5.14.1                 \n",
      "PyQt5-Qt5                         5.15.2                 \n",
      "PyQt5-sip                         12.9.0                 \n",
      "PyQtWebEngine                     5.12                   \n",
      "PyQtWebEngine-Qt5                 5.15.2                 \n",
      "pyRFC3339                         1.1                    \n",
      "pyrsistent                        0.15.5                 \n",
      "pyserial                          3.4                    \n",
      "python-apt                        2.0.0+ubuntu0.20.4.8   \n",
      "python-daemon                     2.2.4                  \n",
      "python-dateutil                   2.7.3                  \n",
      "python-debian                     0.1.36ubuntu1          \n",
      "python-hglib                      2.6.1                  \n",
      "python-utils                      3.3.3                  \n",
      "python-xlib                       0.31                   \n",
      "pytz                              2019.3                 \n",
      "PyWavelets                        1.2.0                  \n",
      "pyxattr                           0.6.1                  \n",
      "PyYAML                            5.3.1                  \n",
      "pyzmq                             18.1.1                 \n",
      "qdldl                             0.1.5.post2            \n",
      "qgrid                             1.3.1                  \n",
      "qtconsole                         5.2.2                  \n",
      "QtPy                              2.0.0                  \n",
      "qtstylish                         0.1.5                  \n",
      "querystring-parser                1.2.4                  \n",
      "rados                             2.0.0                  \n",
      "rbd                               2.0.0                  \n",
      "rdiff-backup                      2.0.0                  \n",
      "regex                             2022.9.13              \n",
      "requests                          2.22.0                 \n",
      "requests-oauthlib                 1.3.0                  \n",
      "requests-unixsocket               0.2.0                  \n",
      "retrying                          1.3.3                  \n",
      "roman                             2.0.0                  \n",
      "rsa                               4.8                    \n",
      "SciencePlots                      1.0.2                  \n",
      "scikit-image                      0.19.0                 \n",
      "scikit-learn                      1.0.2                  \n",
      "scikit-plot                       0.3.7                  \n",
      "scipy                             1.5.4                  \n",
      "screen-resolution-extra           0.0.0                  \n",
      "scs                               3.2.0                  \n",
      "seaborn                           0.11.0                 \n",
      "SecretStorage                     2.3.1                  \n",
      "selinux                           3.0                    \n",
      "Send2Trash                        1.5.0                  \n",
      "service-identity                  18.1.0                 \n",
      "setuptools                        63.1.0                 \n",
      "shap                              0.40.0                 \n",
      "shutilwhich                       1.1.0                  \n",
      "simplejson                        3.16.0                 \n",
      "sip                               4.19.21                \n",
      "six                               1.14.0                 \n",
      "sklearn                           0.0                    \n",
      "skrebate                          0.62                   \n",
      "slicer                            0.0.7                  \n",
      "slip                              0.6.5                  \n",
      "slip.dbus                         0.6.5                  \n",
      "smart-open                        6.2.0                  \n",
      "smmap                             5.0.0                  \n",
      "smmap2                            2.0.5                  \n",
      "sniffio                           1.2.0                  \n",
      "sos                               4.4                    \n",
      "spacy                             2.3.7                  \n",
      "SQLAlchemy                        1.4.41                 \n",
      "sqlparse                          0.4.2                  \n",
      "srsly                             1.0.5                  \n",
      "ssh-import-id                     5.10                   \n",
      "statsmodels                       0.13.2                 \n",
      "sympy                             1.9                    \n",
      "systemd-python                    234                    \n",
      "tabulate                          0.8.9                  \n",
      "tangled-up-in-unicode             0.2.0                  \n",
      "tempdir                           0.7.1                  \n",
      "tenacity                          8.0.1                  \n",
      "tensorboard                       2.7.0                  \n",
      "tensorboard-data-server           0.6.1                  \n",
      "tensorboard-plugin-wit            1.8.1                  \n",
      "tensorflow                        2.7.0                  \n",
      "tensorflow-estimator              2.7.0                  \n",
      "tensorflow-io-gcs-filesystem      0.23.1                 \n",
      "termcolor                         1.1.0                  \n",
      "terminado                         0.12.1                 \n",
      "testpath                          0.4.4                  \n",
      "textblob                          0.17.1                 \n",
      "thinc                             7.4.5                  \n",
      "threadpoolctl                     3.0.0                  \n",
      "tifffile                          2021.11.2              \n",
      "toml                              0.10.2                 \n",
      "tomli                             2.0.0                  \n",
      "torch                             1.12.0                 \n",
      "torchtext                         0.13.1                 \n",
      "torchvision                       0.13.0                 \n",
      "tornado                           6.1                    \n",
      "tqdm                              4.62.3                 \n",
      "traitlets                         5.1.1                  \n",
      "Twisted                           18.9.0                 \n",
      "typing-extensions                 4.0.1                  \n",
      "ubuntu-advantage-tools            27.11.2                \n",
      "ufw                               0.36                   \n",
      "umap-learn                        0.5.3                  \n",
      "unattended-upgrades               0.1                    \n",
      "Unidecode                         1.3.4                  \n",
      "uritemplate                       4.1.1                  \n",
      "urllib3                           1.26.12                \n",
      "vg                                2.0.0                  \n",
      "virtualenv                        20.0.17                \n",
      "visions                           0.7.5                  \n",
      "wadllib                           1.3.3                  \n",
      "wasabi                            0.10.1                 \n",
      "wcwidth                           0.1.8                  \n",
      "webencodings                      0.5.1                  \n",
      "websocket-client                  1.2.1                  \n",
      "Werkzeug                          2.2.2                  \n",
      "wheel                             0.34.2                 \n",
      "widgetsnbextension                3.5.2                  \n",
      "witwidget                         1.8.1                  \n",
      "wordcloud                         1.8.1                  \n",
      "wrapt                             1.13.3                 \n",
      "xdot                              1.1                    \n",
      "xgboost                           1.5.0                  \n",
      "xkit                              0.0.0                  \n",
      "xlrd                              2.0.1                  \n",
      "xport                             3.6.1                  \n",
      "yellowbrick                       1.5                    \n",
      "zipp                              1.0.0                  \n",
      "zope.interface                    4.7.1                  \n"
     ]
    }
   ],
   "source": [
    "!pip list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
